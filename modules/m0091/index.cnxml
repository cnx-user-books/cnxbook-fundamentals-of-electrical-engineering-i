<document xmlns="http://cnx.rice.edu/cnxml" xmlns:m="http://www.w3.org/1998/Math/MathML">
  
  <title>Source Coding Theorem</title>

  <metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m0091</md:content-id>
  <md:title>Source Coding Theorem</md:title>
  <md:abstract>The Source Coding Theorem states that the entropy of an alphabet of symbols specifies to within one bit how many bits on the average need to be used to send the alphabet.</md:abstract>
  <md:uuid>3f8367f6-9946-4ba7-9d06-f9871db92345</md:uuid>
</metadata>

  <content>
    <para id="para1">
      The significance of an alphabet's entropy rests in how we can
      represent it with a sequence of <term>bits</term>.  Bit
      sequences form the "coin of the realm" in digital
      communications: they are the universal way of representing
      symbolic-valued signals.  We convert back and forth between
      symbols to bit-sequences with what is known as a
      <term>codebook</term>: a table that associates symbols to bit
      sequences.  In creating this table, we must be able to assign a
      <emphasis>unique</emphasis> bit sequence to each symbol so that
      we can go between symbol and bit sequences without error.
      <note id="id1169964578953" type="Point of Interest"><label>Point of Interest</label>
	You may be conjuring the notion of hiding information from
	others when we use the name codebook for the
	symbol-to-bit-sequence table.  There is no relation to
	cryptology, which comprises mathematically provable methods of
	securing information.  The codebook terminology was developed
	during the beginnings of information theory just after World
	War II.
      </note>
    </para>
    
    <para id="para2">As we shall explore in some detail elsewhere, <link document="m0519" strength="2">digital communication</link> is
	the transmission of symbolic-valued signals from one place to
	another.  When faced with the problem, for example, of sending
	a file across the Internet, we must first represent each
	character by a bit sequence. Because we want to send the file
	quickly, we want to use as few bits as possible.  However, we
	don't want to use so few bits that the receiver cannot
	determine what each character was from the bit sequence.  For
	example, we could use one bit for every character: File
	transmission would be fast but useless because the codebook
	creates errors. Shannon
	proved in his monumental work what we call today the
	<term>Source Coding Theorem</term>.  Let
      
      <m:math display="inline">
	<m:apply>
	  <m:ci type="fn">B</m:ci>
	  <m:ci>
	    <m:msub>
	      <m:mi>a</m:mi>
	      <m:mi>k</m:mi>
	    </m:msub>
	  </m:ci>
	</m:apply>
      </m:math>
      
      denote the number of bits used to represent the symbol  
      
      <m:math display="inline">
	<m:ci>
	  <m:msub>
	    <m:mi>a</m:mi>
	    <m:mi>k</m:mi>
	  </m:msub>
	</m:ci>
      </m:math>. The average number of bits
      
      <m:math display="inline">
	<m:apply>
	  <m:mean/>
	  <m:apply>
	    <m:ci type="fn">B</m:ci>
	    <m:ci>A</m:ci>
	  </m:apply>
	</m:apply>
      </m:math>

      required to represent the entire alphabet equals
      
      <m:math display="inline">
	<m:apply>
	  <m:sum/>
	  <m:bvar><m:ci>k</m:ci></m:bvar>
	  <m:lowlimit><m:cn>1</m:cn></m:lowlimit>
	  <m:uplimit><m:ci>K</m:ci></m:uplimit>
	  <m:apply>
	    <m:times/>
	    <m:apply>
	      <m:ci type="fn">B</m:ci>
	      <m:ci>
		<m:msub>
		  <m:mi>a</m:mi>
		  <m:mi>k</m:mi>
		</m:msub>
	      </m:ci>
	    </m:apply>
	    <m:apply>
	      <m:csymbol definitionURL="http://cnx.rice.edu/cd/cnxmath.ocd#probability"/>
	      <m:ci>
		<m:msub>
		  <m:mi>a</m:mi>
		  <m:mi>k</m:mi>
		</m:msub>
	      </m:ci>
	    </m:apply>
	  </m:apply>
	</m:apply>
      </m:math>.  <emphasis>The Source Coding Theorem states that the
      average number of bits needed to</emphasis> accurately
      <emphasis>represent the alphabet need only to satisfy</emphasis>
      
      <equation id="eq7">
	<m:math display="block">
	  <m:apply>
	    <m:lt/>
	    <m:apply>
	      <m:leq/>
	      <m:apply>
		<m:ci type="fn">H</m:ci>
		<m:ci>A</m:ci>
	      </m:apply>
	      <m:apply>
		<m:mean/>
		<m:apply>
		  <m:ci type="fn">B</m:ci>
		  <m:ci>A</m:ci>
		</m:apply>
	      </m:apply>
	    </m:apply>
	    <m:apply>
	      <m:plus/>	 
	      <m:apply>
		<m:ci type="fn">H</m:ci>
		<m:ci>A</m:ci>
	      </m:apply>
	      <m:cn>1</m:cn>
	    </m:apply>
	  </m:apply>
	</m:math>
      </equation>

      Thus, the alphabet's entropy specifies to within one bit how
      many bits on the average need to be used to send the alphabet.
      The smaller an alphabet's entropy, the fewer bits required for
      digital transmission of files expressed in that alphabet.
    </para><example id="ex2"> 
      <para id="para3">A four-symbol alphabet has the following probabilities.  
	  <m:math display="block">
	    <m:apply>
	      <m:eq/><m:apply>
	      <m:csymbol definitionURL="http://cnx.rice.edu/cd/cnxmath.ocd#probability"/>
		<m:ci><m:msub><m:mi>a</m:mi><m:mn>0</m:mn></m:msub></m:ci>
	      </m:apply>
	      <m:apply><m:divide/>
		  <m:cn>1</m:cn>
		  <m:cn>2</m:cn>
	      </m:apply>
             </m:apply>
           </m:math>

	  <m:math display="block">
	    <m:apply>
	      <m:eq/><m:apply>
	      <m:csymbol definitionURL="http://cnx.rice.edu/cd/cnxmath.ocd#probability"/>
		<m:ci><m:msub><m:mi>a</m:mi><m:mn>1</m:mn></m:msub></m:ci>
	      </m:apply>
	      <m:apply><m:divide/>
		  <m:cn>1</m:cn>
		  <m:cn>4</m:cn>
	      </m:apply>
             </m:apply>
           </m:math>

	  <m:math display="block">
	    <m:apply>
	      <m:eq/><m:apply>
	      <m:csymbol definitionURL="http://cnx.rice.edu/cd/cnxmath.ocd#probability"/>
		<m:ci><m:msub><m:mi>a</m:mi><m:mn>2</m:mn></m:msub></m:ci>
	      </m:apply>
	      <m:apply><m:divide/>
		  <m:cn>1</m:cn>
		  <m:cn>8</m:cn>
	      </m:apply>
             </m:apply>
           </m:math>

	  <m:math display="block">
	    <m:apply>
	      <m:eq/><m:apply>
	      <m:csymbol definitionURL="http://cnx.rice.edu/cd/cnxmath.ocd#probability"/>
		<m:ci><m:msub><m:mi>a</m:mi><m:mn>3</m:mn></m:msub></m:ci>
	      </m:apply>
	      <m:apply><m:divide/>
		  <m:cn>1</m:cn>
		  <m:cn>8</m:cn>
	      </m:apply>
             </m:apply>
           </m:math>

	and an <link document="m0070" target-id="ex1" strength="2">entropy
	of 1.75 bits</link>.  Let's see if we can find a codebook for
	this four-letter alphabet that satisfies the Source Coding
	Theorem. The simplest code to try is known as the <term>simple
	binary code</term>: convert the symbol's index into a binary
	number and use the same number of bits for each symbol by
	including leading zeros where necessary.
	<equation id="eq8">
	  <m:math>
	    <m:apply>
	      <m:ci><m:mo>↔</m:mo></m:ci>
	      <m:ci><m:msub>
		  <m:mi>a</m:mi>
		  <m:mn>0</m:mn>
		</m:msub></m:ci>
	      <m:ci>00</m:ci>
	    </m:apply>
	    <m:mtext>  </m:mtext>
	    <m:apply>
	      <m:ci><m:mo>↔</m:mo></m:ci>
	      <m:ci><m:msub>
		  <m:mi>a</m:mi>
		  <m:mn>1</m:mn>
		</m:msub></m:ci>
	      <m:ci>01</m:ci>
	    </m:apply>
	    <m:mtext>  </m:mtext>
	    <m:apply>
	      <m:ci><m:mo>↔</m:mo></m:ci>
	      <m:ci><m:msub>
		  <m:mi>a</m:mi>
		  <m:mn>2</m:mn>
		</m:msub></m:ci>
	      <m:ci>10</m:ci>
	    </m:apply>
	    <m:mtext>  </m:mtext>
	    <m:apply>
	      <m:ci><m:mo>↔</m:mo></m:ci>
	      <m:ci><m:msub>
		  <m:mi>a</m:mi>
		  <m:mn>3</m:mn>
		</m:msub></m:ci>
	      <m:ci>11</m:ci>
	    </m:apply>
	  </m:math>
	</equation>
	
	Whenever the number of symbols in the alphabet is a power of
	two (as in this case), the average number of bits
	<m:math display="inline">
	  <m:apply>
	    <m:mean/>
	    <m:apply>
	      <m:ci type="fn">B</m:ci>
	      <m:ci>A</m:ci>
	    </m:apply>
	  </m:apply>
	</m:math>

	equals

	<m:math display="inline">
	  <m:apply>
	    <m:log/>
	    <m:logbase>
	      <m:cn>2</m:cn>
	    </m:logbase>
	    <m:ci>K</m:ci>
	  </m:apply>
	</m:math>, which equals <m:math><m:cn>2</m:cn></m:math>
	in this case.  Because the entropy equals <m:math display="inline"><m:cn>1.75</m:cn></m:math>bits, the simple
	binary code indeed satisfies the Source Coding Theorem—we are
	within one bit of the entropy limit—but you might wonder if
	you can do better.  If we choose a codebook with differing
	number of bits for the symbols, a smaller average number of
	bits can indeed be obtained.  The idea is to use shorter bit
	sequences for the symbols that occur more often.  One codebook
	like this is
	<equation id="eq9">
	  <m:math> 
	    <m:apply>
	      <m:ci><m:mo>↔</m:mo></m:ci>
	      <m:ci><m:msub>
		  <m:mi>a</m:mi>
		  <m:mn>0</m:mn>
		</m:msub></m:ci>
	      <m:ci>0</m:ci>
	    </m:apply>
	    <m:mtext>  </m:mtext>
	    <m:apply>
	      <m:ci><m:mo>↔</m:mo></m:ci>
	      <m:ci><m:msub>
		  <m:mi>a</m:mi>
		  <m:mn>1</m:mn>
		</m:msub></m:ci>
	      <m:ci>10</m:ci>
	    </m:apply>
	    <m:mtext>  </m:mtext>
	    <m:apply>
	      <m:ci><m:mo>↔</m:mo></m:ci>
	      <m:ci><m:msub>
		  <m:mi>a</m:mi>
		  <m:mn>2</m:mn>
		</m:msub></m:ci>
	      <m:ci>110</m:ci>
	    </m:apply>
	    <m:mtext>  </m:mtext>
	    <m:apply>
	      <m:ci><m:mo>↔</m:mo></m:ci>
	      <m:ci><m:msub>
		  <m:mi>a</m:mi>
		  <m:mn>3</m:mn>
		</m:msub></m:ci>
	      <m:ci>111</m:ci>
	    </m:apply>
	  </m:math>
	</equation>
	Now  

	<m:math display="inline">
	  <m:apply>
	    <m:eq/>
	    <m:apply>
	      <m:mean/>
	      <m:apply>
		<m:ci type="fn">B</m:ci>
		<m:ci>A</m:ci>
	      </m:apply>
	    </m:apply>
	    <m:apply>
	      <m:plus/>
	      <m:apply>
		<m:times/>
		<m:cn>1</m:cn>
		<m:ci>·</m:ci>
                <m:apply><m:divide/>
		  <m:cn>1</m:cn>
		  <m:cn>2</m:cn>
		</m:apply>
	      </m:apply>
	      <m:apply>
		<m:times/>
		<m:cn>2</m:cn>
		<m:ci>·</m:ci>
                <m:apply><m:divide/>
		  <m:cn>1</m:cn>
		  <m:cn>4</m:cn>
		</m:apply>
	      </m:apply> 
	      <m:apply>
		<m:times/>
		<m:cn>3</m:cn>
		<m:ci>·</m:ci>
                <m:apply><m:divide/>
		  <m:cn>1</m:cn>
		  <m:cn>8</m:cn>
		</m:apply>
	      </m:apply> 
	      <m:apply>
		<m:times/>
		<m:cn>3</m:cn>
		<m:ci>·</m:ci>
                <m:apply><m:divide/>
		  <m:cn>1</m:cn>
		  <m:cn>8</m:cn>
		</m:apply>
	      </m:apply> 
	    </m:apply>
	    <m:cn>1.75</m:cn>
	  </m:apply>
	</m:math>.  We can reach the entropy limit! The simple
	binary code is, in this case, less efficient than the
	unequal-length code. Using the efficient code, we can transmit
	the symbolic-valued signal having this alphabet 12.5%
	faster. Furthermore, we know that no more efficient codebook
	can be found because of Shannon's Theorem.
      </para></example>

  </content>
</document>